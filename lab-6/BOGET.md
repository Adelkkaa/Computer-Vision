
### 1. Исходные данные
Предположим, у нас есть шахматная доска размером `3x4` (3 ряда и 4 столбца). Это означает, что на доске будет `3 * 4 = 12` углов.

---

### 2. Создание массива `objp`
```python
objp = np.zeros((chessboard_size[0] * chessboard_size[1], 3), np.float32)
```

#### Что происходит:
- `chessboard_size[0] * chessboard_size[1]` — это общее количество углов на доске (`3 * 4 = 12`).
- Мы создаем массив `objp` размером `(12, 3)`, где каждая строка представляет координаты одного угла в трехмерном пространстве `(X, Y, Z)`.
- Все значения в массиве изначально равны `0`.

#### Результат:
Массив `objp` выглядит так:
```
[
    [0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0],
    ...
    [0.0, 0.0, 0.0]
]
```
(12 строк, каждая с тремя нулями).

---

### 3. Заполнение координат `X` и `Y`
```python
objp[:, :2] = np.mgrid[0:chessboard_size[0], 0:chessboard_size[1]].T.reshape(-1, 2)
```

#### Разбор по частям:

1. **`np.mgrid[0:chessboard_size[0], 0:chessboard_size[1]]`**
   - `np.mgrid` создает сетку координат для заданного диапазона.
   - Для нашей доски размером `3x4` это будет выглядеть так:
     ```
     [[0, 0, 0, 0],
      [1, 1, 1, 1],
      [2, 2, 2, 2]]
     ```
     (это координаты `Y`) и
     ```
     [[0, 1, 2, 3],
      [0, 1, 2, 3],
      [0, 1, 2, 3]]
     ```
     (это координаты `X`).

2. **`.T` (транспонирование)**
   - Мы меняем строки и столбцы местами, чтобы получить следующую структуру:
     ```
     [[0, 0],
      [0, 1],
      [0, 2],
      [0, 3],
      [1, 0],
      [1, 1],
      [1, 2],
      [1, 3],
      [2, 0],
      [2, 1],
      [2, 2],
      [2, 3]]
     ```

3. **`.reshape(-1, 2)`**
   - Преобразуем двумерную матрицу в одномерный список пар `(X, Y)`:
     ```
     [
         [0, 0],
         [0, 1],
         [0, 2],
         [0, 3],
         [1, 0],
         [1, 1],
         [1, 2],
         [1, 3],
         [2, 0],
         [2, 1],
         [2, 2],
         [2, 3]
     ]
     ```

4. **`objp[:, :2] = ...`**
   - Мы присваиваем первые два столбца массива `objp` (координаты `X` и `Y`) значениям из этой сетки.
   - Третий столбец (`Z`) остается равным `0`, так как мы предполагаем, что все углы находятся на одной плоскости.

---

### 4. Итоговый массив `objp`
После выполнения этих операций массив `objp` будет выглядеть так:
```
[
    [0.0, 0.0, 0.0],  # Угол (0, 0)
    [0.0, 1.0, 0.0],  # Угол (0, 1)
    [0.0, 2.0, 0.0],  # Угол (0, 2)
    [0.0, 3.0, 0.0],  # Угол (0, 3)
    [1.0, 0.0, 0.0],  # Угол (1, 0)
    [1.0, 1.0, 0.0],  # Угол (1, 1)
    [1.0, 2.0, 0.0],  # Угол (1, 2)
    [1.0, 3.0, 0.0],  # Угол (1, 3)
    [2.0, 0.0, 0.0],  # Угол (2, 0)
    [2.0, 1.0, 0.0],  # Угол (2, 1)
    [2.0, 2.0, 0.0],  # Угол (2, 2)
    [2.0, 3.0, 0.0]   # Угол (2, 3)
]
```

Каждая строка содержит координаты `(X, Y, Z)` одного угла шахматной доски.

---

### 5. Зачем это нужно?
Этот массив `objp` используется для калибровки камеры. Он представляет реальные координаты углов шахматной доски в мире (в миллиметрах или единицах измерения вашей доски). Когда OpenCV находит эти углы на изображении, он сравнивает их реальные координаты (`objp`) с их пиксельными координатами на изображении, чтобы вычислить параметры камеры.

---

### Пример визуализации

Если представить шахматную доску размером `3x4`, то она будет выглядеть так:

```
(0,0) --- (0,1) --- (0,2) --- (0,3)
   |         |         |         |
(1,0) --- (1,1) --- (1,2) --- (1,3)
   |         |         |         |
(2,0) --- (2,1) --- (2,2) --- (2,3)
```

А массив `objp` просто хранит эти координаты в виде списка.

---

### Вывод
Эти две строки кода создают массив реальных координат углов шахматной доски. Они необходимы для калибровки камеры, так как позволяют связать реальный мир с изображением.

#### Полный код функции:
```python
def calibrate_camera(chessboard_images_path, chessboard_size=(7, 11)):
    objp = np.zeros((chessboard_size[0] * chessboard_size[1], 3), np.float32)
    objp[:, :2] = np.mgrid[0:chessboard_size[0], 0:chessboard_size[1]].T.reshape(-1, 2)

    objpoints = []  # Список для хранения реальных координат углов
    imgpoints = []  # Список для хранения найденных углов на изображениях

    images = glob.glob(os.path.join(chessboard_images_path, '*.png'))  # Находим все PNG-изображения в указанной папке
    for img_path in images:
        img = cv2.imread(img_path, 0)  # Читаем изображение в оттенках серого
        ret, corners = cv2.findChessboardCorners(img, chessboard_size, None)  # Находим углы шахматной доски
        if ret:  # Если углы найдены
            objpoints.append(objp)  # Добавляем реальные координаты
            imgpoints.append(corners)  # Добавляем найденные углы

    if not objpoints or not imgpoints:  # Проверяем, что были найдены углы хотя бы на одном изображении
        print("Не удалось найти углы шахматной доски.")
        return None, None

    ret, camera_matrix, dist_coeffs, _, _ = cv2.calibrateCamera(
        objpoints, imgpoints, img.shape[::-1], None, None
    )  # Выполняем калибровку камеры
    return camera_matrix, dist_coeffs  # Возвращаем матрицу камеры и коэффициенты искажения
```

---

### Разбор кода построчно:

#### 1. Инициализация списков `objpoints` и `imgpoints`
```python
objpoints = []  # Список для хранения реальных координат углов
imgpoints = []  # Список для хранения найденных углов на изображениях
```

- **`objpoints`**: Здесь будут храниться реальные координаты углов шахматной доски (массив `objp`, который мы создали ранее).
- **`imgpoints`**: Здесь будут храниться координаты углов, найденные на изображениях (в пикселях).

Эти два списка нужны для передачи данных в функцию калибровки камеры.

---

#### 2. Поиск изображений шахматной доски
```python
images = glob.glob(os.path.join(chessboard_images_path, '*.png'))  # Находим все PNG-изображения в указанной папке
```

- **`glob.glob`**: Это функция для поиска файлов по шаблону.
- **`os.path.join`**: Составляет путь к файлам, добавляя маску `*.png` (все PNG-файлы).
- Результат — список путей к изображениям шахматной доски.

---

#### 3. Цикл обработки каждого изображения
```python
for img_path in images:
    img = cv2.imread(img_path, 0)  # Читаем изображение в оттенках серого
    ret, corners = cv2.findChessboardCorners(img, chessboard_size, None)  # Находим углы шахматной доски
    if ret:  # Если углы найдены
        objpoints.append(objp)  # Добавляем реальные координаты
        imgpoints.append(corners)  # Добавляем найденные углы
```

- **`cv2.imread(img_path, 0)`**: Загружает изображение в оттенках серого (`0` означает grayscale).
- **`cv2.findChessboardCorners`**: Функция для поиска углов шахматной доски.
  - **`img`**: Изображение, на котором нужно найти углы.
  - **`chessboard_size`**: Размер шахматной доски (например, `(7, 11)`).
  - **`None`**: Параметры алгоритма (по умолчанию).
  - **`ret`**: Логическое значение, указывающее, были ли найдены углы (`True` или `False`).
  - **`corners`**: Координаты найденных углов (в пикселях).

Если углы найдены (`ret == True`):
- Мы добавляем реальные координаты (`objp`) в список `objpoints`.
- Мы добавляем найденные углы (`corners`) в список `imgpoints`.

---

#### 4. Проверка наличия найденных углов
```python
if not objpoints or not imgpoints:  # Проверяем, что были найдены углы хотя бы на одном изображении
    print("Не удалось найти углы шахматной доски.")
    return None, None
```

- Если ни на одном изображении не удалось найти углы (`objpoints` или `imgpoints` пустые), то выходим из функции, возвращая `None`.

---

#### 5. Калибровка камеры
```python
ret, camera_matrix, dist_coeffs, _, _ = cv2.calibrateCamera(
    objpoints, imgpoints, img.shape[::-1], None, None
)  # Выполняем калибровку камеры
```

- **`cv2.calibrateCamera`**: Основная функция для калибровки камеры.
  - **`objpoints`**: Реальные координаты углов шахматной доски.
  - **`imgpoints`**: Найденные углы на изображениях (в пикселях).
  - **`img.shape[::-1]`**: Размер изображения (ширина и высота).
  - **`None`**: Внутренние параметры камеры (неизвестны заранее).
  - **`None`**: Коэффициенты искажения (также неизвестны заранее).

Результаты:
- **`camera_matrix`**: Матрица камеры (описывает фокусное расстояние и положение центра изображения).
- **`dist_coeffs`**: Коэффициенты искажения (описывают баррельные или подушкообразные искажения).

---

#### 6. Возврат результатов
```python
return camera_matrix, dist_coeffs  # Возвращаем матрицу камеры и коэффициенты искажения
```

- Функция возвращает два важных результата:
  - **`camera_matrix`**: Матрица камеры, которая описывает её внутренние параметры.
  - **`dist_coeffs`**: Коэффициенты искажения, которые помогают исправить деформацию изображения.

---

### Пример работы функции

Предположим, у нас есть следующие данные:
- Размер шахматной доски: `(7, 11)`
- 10 изображений шахматной доски.

Функция:
1. Создаст массив `objp` с реальными координатами углов.
2. Найдет углы на каждом изображении.
3. Сохранит реальные и найденные координаты в списках `objpoints` и `imgpoints`.
4. Выполнит калибровку камеры, используя эти данные.
5. Вернет матрицу камеры и коэффициенты искажения.

---

### Зачем это нужно?

Калибровка камеры позволяет:
- Узнать точные параметры камеры (фокусное расстояние, положение центра изображения).
- Исправить искажения (например, баррельные искажения, когда прямые линии кажутся弯曲ыми).
- Подготовить камеру для дальнейшей работы (например, стереовидения или аугментированной реальности). 

Давайте разберем **стереокалибровку** пошагово, чтобы понять, как она работает и зачем нужна. Стереокалибровка — это процесс определения взаимного положения и ориентации двух камер (или одного камеры в разных положениях), которые снимают одну и ту же сцену.

---

### Что такое стереокалибровка?

Стереокалибровка позволяет найти:
1. **Матрицу вращения (`R`)**: Как одна камера повернута относительно другой.
2. **Вектор смещения (`T`)**: Насколько одна камера смещена относительно другой в пространстве.
3. Эти параметры необходимы для того, чтобы объединить изображения с двух камер в одну стереосистему, которая может использоваться для построения трехмерных моделей или расчета глубины.

---

### Код функции `stereo_calibrate`

#### Полный код функции:
```python
def stereo_calibrate(camera_matrix, dist_coeffs, imgL, imgR):
    sift = cv2.SIFT_create()  # Создаем объект SIFT для детектирования ключевых точек
    kp1, des1 = sift.detectAndCompute(imgL, None)  # Находим ключевые точки и их описатели на левом изображении
    kp2, des2 = sift.detectAndCompute(imgR, None)  # Находим ключевые точки и их описатели на правом изображении

    bf = cv2.BFMatcher()  # Создаем объект BFMatcher для сопоставления описателей
    matches = bf.knnMatch(des1, des2, k=2)  # Находим две лучших соответствия для каждой точки

    good_matches = [m for m, n in matches if m.distance < 0.7 * n.distance]  # Отфильтровываем плохие соответствия

    if len(good_matches) < 8:  # Проверяем, что есть достаточно хороших соответствий
        print("Недостаточно совпадений.")
        return None, None

    pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # Координаты точек на левом изображении
    pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # Координаты точек на правом изображении

    F, _ = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC)  # Вычисляем фундаментальную матрицу
    E = camera_matrix.T @ F @ camera_matrix  # Вычисляем эссенциальную матрицу
    _, R, T, _ = cv2.recoverPose(E, pts1, pts2, camera_matrix)  # Вычисляем матрицу вращения и вектор смещения

    return R, T  # Возвращаем матрицу вращения и вектор смещения
```

---

### Разбор кода построчно:

#### 1. Детектирование ключевых точек
```python
sift = cv2.SIFT_create()  # Создаем объект SIFT для детектирования ключевых точек
kp1, des1 = sift.detectAndCompute(imgL, None)  # Находим ключевые точки и их описатели на левом изображении
kp2, des2 = sift.detectAndCompute(imgR, None)  # Находим ключевые точки и их описатели на правом изображении
```

- **`cv2.SIFT_create()`**: SIFT (Scale-Invariant Feature Transform) — алгоритм для обнаружения ключевых точек и их описателей.
- **`detectAndCompute`**:
  - Находит ключевые точки (`kp1`, `kp2`) на обоих изображениях.
  - Вычисляет описатели (`des1`, `des2`) для каждой точки. Описатель — это числовое представление окрестности точки, которое позволяет сравнивать точки между изображениями.

---

#### 2. Сопоставление точек
```python
bf = cv2.BFMatcher()  # Создаем объект BFMatcher для сопоставления описателей
matches = bf.knnMatch(des1, des2, k=2)  # Находим две лучших соответствия для каждой точки
```

- **`cv2.BFMatcher`**: Brute-Force Matcher — простой алгоритм для сопоставления описателей.
- **`knnMatch`**:
  - Для каждой точки на левом изображении находит `k=2` ближайших соответствий на правом изображении.
  - Это позволяет отфильтровать плохие соответствия позже.

---

#### 3. Фильтрация соответствий
```python
good_matches = [m for m, n in matches if m.distance < 0.7 * n.distance]  # Отфильтровываем плохие соответствия
```

- **Правило Лоу (Low's ratio test)**:
  - Если расстояние до лучшего соответствия (`m.distance`) меньше 0.7 расстояния до второго лучшего соответствия (`n.distance`), считаем это хорошим соответствием.
  - Это помогает исключить ошибочные соответствия.

---

#### 4. Проверка количества соответствий
```python
if len(good_matches) < 8:  # Проверяем, что есть достаточно хороших соответствий
    print("Недостаточно совпадений.")
    return None, None
```

- Для вычисления фундаментальной матрицы нужно минимум 8 хороших соответствий.
- Если их меньше, то калибровка невозможна.

---

#### 5. Преобразование соответствий в координаты
```python
pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # Координаты точек на левом изображении
pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # Координаты точек на правом изображении
```

- **`kp1[m.queryIdx].pt`**: Получаем координаты соответствующей точки на левом изображении.
- **`kp2[m.trainIdx].pt`**: Получаем координаты соответствующей точки на правом изображении.
- Результаты преобразуются в массивы NumPy размером `(N, 1, 2)`, где `N` — количество соответствий.

---

#### 6. Вычисление фундаментальной матрицы
```python
F, _ = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC)  # Вычисляем фундаментальную матрицу
```

- **Фундаментальная матрица (`F`)**:
  - Описывает геометрические отношения между двумя изображениями.
  - Соотносит точки на одном изображении с точками на другом.
- **`cv2.FM_RANSAC`**: Метод RANSAC используется для исключения ошибочных соответствий.

---

#### 7. Вычисление эссенциальной матрицы
```python
E = camera_matrix.T @ F @ camera_matrix  # Вычисляем эссенциальную матрицу
```

- **Эссенциальная матрица (`E`)**:
  - Похожа на фундаментальную, но учитывает внутренние параметры камеры (`camera_matrix`).
  - Формула: \( E = K^T \cdot F \cdot K \), где \( K \) — матрица камеры.

---

#### 8. Вычисление матрицы вращения и вектора смещения
```python
_, R, T, _ = cv2.recoverPose(E, pts1, pts2, camera_matrix)  # Вычисляем матрицу вращения и вектор смещения
```

- **`cv2.recoverPose`**:
  - Использует эссенциальную матрицу для вычисления:
    - **`R`**: Матрица вращения (описывает, как одна камера повернута относительно другой).
    - **`T`**: Вектор смещения (описывает, как одна камера смещена относительно другой).

---

#### 9. Возврат результатов
```python
return R, T  # Возвращаем матрицу вращения и вектор смещения
```

- Эти параметры (`R` и `T`) нужны для дальнейшей работы со стереосистемой, например, для ректификации изображений.

---

### Пример работы функции

Предположим, у нас есть два изображения:
- Левое изображение (`imgL`): Снято одной камерой.
- Правое изображение (`imgR`): Снято второй камерой.

Функция:
1. Находит ключевые точки и их описатели на обоих изображениях.
2. Сопоставляет эти точки.
3. Вычисляет фундаментальную и эссенциальную матрицы.
4. Определяет матрицу вращения (`R`) и вектор смещения (`T`).

---

### Зачем это нужно?

Результаты стереокалибровки (`R` и `T`) используются для:
1. **Ректификации изображений**: Приведения эпиполярных линий к горизонтальному виду, что упрощает расчет глубины.
2. **Строения трехмерных моделей**: Использования информации о положении камер для восстановления геометрии сцены.
3. **Вычисления карт глубины**: Определения расстояния до объектов на основе различий между изображениями.

Ректификация изображений — это процесс преобразования двух стереоизображений так, чтобы эпиполярные линии (линии, по которым могут находиться соответствующие точки на обоих изображениях) стали параллельными и горизонтальными. Это упрощает дальнейший поиск соответствий между точками на изображениях, что необходимо для расчета глубины или построения трехмерных моделей.

---

### Что делает ректификация?

1. **Выравнивание эпиполярных линий**: После ректификации все соответствующие точки на изображениях будут находиться на одной и той же горизонтальной линии.
2. **Исправление искажений**: Устраняются перспективные искажения, вызванные различным положением камер.
3. **Подготовка к расчету глубины**: Ректифицированные изображения позволяют эффективно вычислять карты глубины с использованием методов стереосопоставления.

---

### Код функции `rectify_images`

#### Полный код функции:
```python
def rectify_images(imgL, imgR, camera_matrix, dist_coeffs, R, T):
    h, w = imgL.shape  # Получаем размеры изображения
    R1, R2, P1, P2, Q, _, _ = cv2.stereoRectify(
        camera_matrix, dist_coeffs, camera_matrix, dist_coeffs, (w, h), R, T
    )  # Вычисляем новые матрицы проекции и матрицы ректификации

    map1L, map2L = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, R1, P1, (w, h), cv2.CV_32FC1)  # Создаем карты преобразования для левого изображения
    map1R, map2R = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, R2, P2, (w, h), cv2.CV_32FC1)  # Создаем карты преобразования для правого изображения

    rectified_imgL = cv2.remap(imgL, map1L, map2L, cv2.INTER_LINEAR)  # Применяем карты преобразования к левому изображению
    rectified_imgR = cv2.remap(imgR, map1R, map2R, cv2.INTER_LINEAR)  # Применяем карты преобразования к правому изображению

    return rectified_imgL, rectified_imgR  # Возвращаем откалиброванные изображения
```

---

### Разбор кода построчно:

#### 1. Получение размеров изображения
```python
h, w = imgL.shape  # Получаем размеры изображения
```

- **`imgL.shape`**: Возвращает размеры изображения в формате `(высота, ширина)`.
- Переменные `h` и `w` хранят высоту и ширину изображения соответственно.

---

#### 2. Вычисление новых матриц проекции и ректификации
```python
R1, R2, P1, P2, Q, _, _ = cv2.stereoRectify(
    camera_matrix, dist_coeffs, camera_matrix, dist_coeffs, (w, h), R, T
)
```

- **`cv2.stereoRectify`**: Функция для вычисления новых матриц проекции (`P1`, `P2`) и матриц ректификации (`R1`, `R2`).
- Параметры:
  - **`camera_matrix`**: Матрица камеры для обеих камер (обычно одинаковая, если камеры одинаковые).
  - **`dist_coeffs`**: Коэффициенты искажения для обеих камер.
  - **`(w, h)`**: Размер изображения.
  - **`R`**: Матрица вращения, полученная при стереокалибровке.
  - **`T`**: Вектор смещения, полученный при стереокалибровке.
- Результаты:
  - **`R1`, `R2`**: Новые матрицы вращения для левой и правой камер.
  - **`P1`, `P2`**: Новые матрицы проекции для левой и правой камер.
  - **`Q`**: Матрица ректификации, используемая для преобразования координат пикселей в трехмерные координаты.

---

#### 3. Создание карт преобразования
```python
map1L, map2L = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, R1, P1, (w, h), cv2.CV_32FC1)  # Для левого изображения
map1R, map2R = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, R2, P2, (w, h), cv2.CV_32FC1)  # Для правого изображения
```

- **`cv2.initUndistortRectifyMap`**: Создает карты преобразования для исправления искажений и применения ректификации.
- Параметры:
  - **`camera_matrix`**: Матрица камеры.
  - **`dist_coeffs`**: Коэффициенты искажения.
  - **`R1`, `R2`**: Матрицы ректификации для левой и правой камер.
  - **`P1`, `P2`**: Матрицы проекции для левой и правой камер.
  - **`(w, h)`**: Размер изображения.
  - **`cv2.CV_32FC1`**: Тип данных для карт преобразования (32-битное число с плавающей запятой).
- Результаты:
  - **`map1L`, `map2L`**: Карты преобразования для левого изображения.
  - **`map1R`, `map2R`**: Карты преобразования для правого изображения.

---

#### 4. Применение карт преобразования
```python
rectified_imgL = cv2.remap(imgL, map1L, map2L, cv2.INTER_LINEAR)  # Для левого изображения
rectified_imgR = cv2.remap(imgR, map1R, map2R, cv2.INTER_LINEAR)  # Для правого изображения
```

- **`cv2.remap`**: Применяет карты преобразования к изображению.
- Параметры:
  - **`imgL`, `imgR`**: Исходные изображения.
  - **`map1L`, `map2L`**: Карты преобразования для левого изображения.
  - **`map1R`, `map2R`**: Карты преобразования для правого изображения.
  - **`cv2.INTER_LINEAR`**: Метод интерполяции для преобразования пикселей.
- Результаты:
  - **`rectified_imgL`**: Откалиброванное левое изображение.
  - **`rectified_imgR`**: Откалиброванное правое изображение.

---

#### 5. Возврат результатов
```python
return rectified_imgL, rectified_imgR  # Возвращаем откалиброванные изображения
```

- Функция возвращает два ректифицированных изображения: левое (`rectified_imgL`) и правое (`rectified_imgR`).

---

### Пример работы функции

Предположим, у нас есть два изображения:
- Левое изображение (`imgL`): Снято одной камерой.
- Правое изображение (`imgR`): Снято второй камерой.

Функция:
1. Вычисляет новые матрицы проекции (`P1`, `P2`) и матрицы ректификации (`R1`, `R2`).
2. Создает карты преобразования для обоих изображений.
3. Применяет эти карты к исходным изображениям.
4. Возвращает два ректифицированных изображения, на которых эпиполярные линии параллельны и горизонтальны.

---

### Зачем это нужно?

Ректифицированные изображения используются для:
1. **Вычисления карт глубины**: После ректификации поиск соответствий между точками сводится к сравнению пикселей только по горизонтали, что значительно упрощает алгоритмы.
2. **Строения трехмерных моделей**: Ректификация помогает точно определить положение объектов в пространстве.
3. **Улучшения качества стереоизображений**: Исправление искажений и выравнивание изображений делает их более удобными для анализа.
